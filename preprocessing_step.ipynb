{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading r/KoreanFood data...\n",
      "Loaded 500 posts from r/KoreanFood.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Loading r/KoreanFood data...\")\n",
    "try:\n",
    "\n",
    "    df_kf = pd.read_csv(\"korean_food_subreddit_raw.csv\")\n",
    "    print(f\"Loaded {len(df_kf)} posts from r/KoreanFood.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'korean_food_subreddit_raw.csv' not found. Did you save it?\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'full_text' column prepared.\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Handle Missing Values & Ensure 'full_text' Exists ---\n",
    "if 'full_text' not in df_kf.columns:\n",
    "    df_kf['title'] = df_kf['title'].fillna('').astype(str)\n",
    "    df_kf['selftext'] = df_kf['selftext'].fillna('').astype(str)\n",
    "    df_kf['comments'] = df_kf['comments'].fillna('').astype(str)\n",
    "    df_kf['full_text'] = df_kf['title'] + \" \" + df_kf['selftext'] + \" \" + df_kf['comments']\n",
    "else:\n",
    "    df_kf['full_text'] = df_kf['full_text'].fillna('').astype(str)\n",
    "print(\"'full_text' column prepared.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning r/KoreanFood text...\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Define Cleaning Function (Same as before) ---\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\@\\w+|\\#', '', text)\n",
    "    text = re.sub(r'\\|', ' ', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "print(\"Cleaning r/KoreanFood text...\")\n",
    "df_kf['cleaned_text'] = df_kf['full_text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing r/KoreanFood text (NLTK)...\n",
      "\n",
      "Preprocessing Complete! Here's a sample:\n",
      "                                               title  \\\n",
      "0  Every Korean mom has made this for their child...   \n",
      "1  My girl made me dinner, she cooks me dinner on...   \n",
      "2                  Beef Bulgogi and homemade banchan   \n",
      "3     Gimbap for my sons first day at his new school   \n",
      "4                                  How is my Kimbap?   \n",
      "\n",
      "                                    processed_tokens  \n",
      "0  [mom, child, point, parent, family, poor, pare...  \n",
      "1  [girl, month, shes, meal, material, right, mea...  \n",
      "2  [beef, bulgogi, homemade, banchan, god, man, j...  \n",
      "3  [gimbap, son, new, school, husband, son, secon...  \n",
      "4  [kimbap, ingredient, imitation, crab, egg, fis...  \n",
      "\n",
      "Saved processed r/KoreanFood data to 'korean_food_subreddit_processed.csv'\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Define NLTK Processing Function (Using Expanded Stopwords) ---\n",
    "stop_words = set(stopwords.words('english'))\n",
    "custom_stopwords_layer1 = [\n",
    "    'korea', 'korean', 'seoul', 'visit', 'trip', 'travel', 'tourist',\n",
    "    'food', 'eat', 'restaurant', 'place', 'market', 'drink',\n",
    "    'recommend', 'suggestion', 'anyone', 'know', 'thanks', 'thank',\n",
    "    'help', 'advice', 'question', 'please', 'looking', 'wondering',\n",
    "    'like', 'good', 'nice', 'great', 'amazing', 'delicious', 'best',\n",
    "    'really', 'also', 'get', 'go', 'try', 'went', 'im', 'ive', 'im',\n",
    "    'one', 'day', 'time', 'people', 'lot', 'would', 'us', 'got', 'much',\n",
    "    'experience', 'english', 'sure', 'want', 'think', 'things', 'make',\n",
    "    'made', 'recipe', 'cook', 'cooking' # Added some cooking words\n",
    "]\n",
    "custom_stopwords_layer2 = [\n",
    "    'station', 'hotel', 'street', 'museum', 'myeongdong', 'area', 'park',\n",
    "    'city', 'night', 'busan', 'village', 'tour', 'bus', 'hour', 'many',\n",
    "    'palace', 'thing', 'map', 'cafe', 'store', 'airport', 'dinner', 'way',\n",
    "    'hongdae', 'taxi', 'card', 'line', 'itinerary', 'local', 'walk', 'train',\n",
    "    'jeju', 'island', 'ticket', 'need', 'plan', 'check', 'book', 'look',\n",
    "    'youre', 'first', 'shopping', 'kid', 'free', 'beach', 'hanok', 'morning',\n",
    "    'option', 'google', 'friend', 'traditional', 'open', 'flight', 'small',\n",
    "    'temple', 'last', 'bit', 'app', 'tower', 'view', 'dont', 'car', 'stop'\n",
    "]\n",
    "stop_words.update(custom_stopwords_layer1)\n",
    "stop_words.update(custom_stopwords_layer2)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'): return nltk.corpus.wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'): return nltk.corpus.wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'): return nltk.corpus.wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'): return nltk.corpus.wordnet.ADV\n",
    "    else: return nltk.corpus.wordnet.NOUN\n",
    "\n",
    "def process_text_reddit(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    processed_tokens = []\n",
    "    for word, tag in pos_tags:\n",
    "        if tag.startswith('NN') or tag.startswith('JJ'):\n",
    "            wn_tag = get_wordnet_pos(tag)\n",
    "            lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "            if lemma not in stop_words and len(lemma) > 2:\n",
    "                processed_tokens.append(lemma)\n",
    "    return processed_tokens\n",
    "\n",
    "print(\"Processing r/KoreanFood text (NLTK)...\")\n",
    "df_kf['processed_tokens'] = df_kf['cleaned_text'].apply(process_text_reddit)\n",
    "\n",
    "print(\"\\nPreprocessing Complete! Here's a sample:\")\n",
    "print(df_kf[['title', 'processed_tokens']].head())\n",
    "\n",
    "\n",
    "try:\n",
    "\n",
    "    df_kf.to_csv(\"korean_food_subreddit_processed.csv\", index=False)\n",
    "    print(\"\\nSaved processed r/KoreanFood data to 'korean_food_subreddit_processed.csv'\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError saving CSV: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
