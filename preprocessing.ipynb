{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in /opt/anaconda3/envs/ml_base/lib/python3.10/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/envs/ml_base/lib/python3.10/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/envs/ml_base/lib/python3.10/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/ml_base/lib/python3.10/site-packages (from nltk) (4.67.1)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.9.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  1. Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4254 articles.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"south_korea_news_all.csv\")\n",
    "print(f\"Loaded {len(df)} articles.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Handle Missing Values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['headline'] = df['headline'].fillna('').astype(str)\n",
    "df['snippet'] = df['snippet'].fillna('').astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Combine Text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined headline and snippet into 'full_text'.\n"
     ]
    }
   ],
   "source": [
    "df['full_text'] = df['headline'] + \" \" + df['snippet']\n",
    "print(\"Combined headline and snippet into 'full_text'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Define Cleaning Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning text (lowercase, removing URLs, punctuation, numbers)...\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower() # Lowercase\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE) # Remove URLs\n",
    "    text = re.sub(r'\\@\\w+|\\#', '', text) # Removing @mentions and #\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # Removing punctuation\n",
    "    text = re.sub(r'\\d+', '', text) # Removing numbers\n",
    "    return text\n",
    "\n",
    "print(\"Cleaning text (lowercase, removing URLs, punctuation, numbers)...\")\n",
    "df['cleaned_text'] = df['full_text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/user/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /Users/user/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/user/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/user/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5-8. Define NLTK Processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text (tokenizing, POS tagging, filtering Nouns/Adjectives, lemmatizing, removing stopwords)...\n",
      "\n",
      "Preprocessing Complete! Here's a sample:\n",
      "         source                                           headline  \\\n",
      "0  The Guardian  Trump v the world: Inside the 3 January Guardi...   \n",
      "1  The Guardian  South Korea plane crash investigators turn to ...   \n",
      "2  The Guardian  South Korea plane crash investigations focus o...   \n",
      "3  The Guardian  ‘Sex strikes’ aren’t the feminist win they app...   \n",
      "4  The Guardian  Green light: the boss of GB Railfreight with a...   \n",
      "\n",
      "                                    processed_tokens  \n",
      "0  [trump, world, january, guardian, weekly, glob...  \n",
      "1  [korea, plane, crash, investigator, black, box...  \n",
      "2  [korea, plane, crash, investigation, role, air...  \n",
      "3  [sex, strike, feminist, radical, problem, move...  \n",
      "4  [green, bos, railfreight, eye, environment, te...  \n"
     ]
    }
   ],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return nltk.corpus.wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return nltk.corpus.wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return nltk.corpus.wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return nltk.corpus.wordnet.ADV\n",
    "    else:\n",
    "        return nltk.corpus.wordnet.NOUN # Default to noun\n",
    "\n",
    "def process_text(text):\n",
    "    tokens = word_tokenize(text) # 5. Tokenize\n",
    "    pos_tags = nltk.pos_tag(tokens) # 6. POS Tag\n",
    "    \n",
    "    processed_tokens = []\n",
    "    for word, tag in pos_tags:\n",
    "        # 6. Filter for Nouns (NN) and Adjectives (JJ)\n",
    "        if tag.startswith('NN') or tag.startswith('JJ'):\n",
    "            # 7. Lemmatize\n",
    "            wn_tag = get_wordnet_pos(tag)\n",
    "            lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "            # 8. Remove Stopwords & Short Words\n",
    "            if lemma not in stop_words and len(lemma) > 2:\n",
    "                processed_tokens.append(lemma)\n",
    "                \n",
    "    return processed_tokens\n",
    "\n",
    "print(\"Processing text (tokenizing, POS tagging, filtering Nouns/Adjectives, lemmatizing, removing stopwords)...\")\n",
    "# This step can take a few minutes for 4000+ articles\n",
    "df['processed_tokens'] = df['cleaned_text'].apply(process_text)\n",
    "\n",
    "print(\"\\nPreprocessing Complete! Here's a sample:\")\n",
    "print(df[['source', 'headline', 'processed_tokens']].head())\n",
    "\n",
    "# --- Optional: Save the Processed Data ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved processed data to 'south_korea_news_processed.csv'\n"
     ]
    }
   ],
   "source": [
    "df.to_csv(\"south_korea_news_processed.csv\", index=False)\n",
    "print(\"\\nSaved processed data to 'south_korea_news_processed.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
