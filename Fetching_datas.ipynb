{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Guardian News ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GUARDIAN_API_KEY = \"6bbee1ea-b550-4b54-b3b1-697aaa0962db\"  # <--- MY KEY HERE!\n",
    "GUARDIAN_API_URL = \"https://content.guardianapis.com/search\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_guardian_articles(api_key, query, from_date, to_date, page=1):\n",
    "    \"\"\"Fetches articles from The Guardian API.\"\"\"\n",
    "    params = {\n",
    "        \"q\": query,\n",
    "        \"api-key\": api_key,\n",
    "        \"from-date\": from_date, # Format: YYYY-MM-DD\n",
    "        \"to-date\": to_date,     # Format: YYYY-MM-DD\n",
    "        \"show-fields\": \"headline,trailText,publication\", # Get headline and snippet (trailText)\n",
    "        \"page-size\": 50, # Max allowed is 50\n",
    "        \"page\": page,\n",
    "        \"order-by\": \"newest\"\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(GUARDIAN_API_URL, params=params)\n",
    "        response.raise_for_status()  # Raise an exception for bad status codes\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching Guardian data: {e}\")\n",
    "        return None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching from The Guardian...\n",
      "  Page 1/65 fetched, 50 articles.\n",
      "  Page 2/65 fetched, 50 articles.\n",
      "  Page 3/65 fetched, 50 articles.\n",
      "  Page 4/65 fetched, 50 articles.\n",
      "  Page 5/65 fetched, 50 articles.\n",
      "  Page 6/65 fetched, 50 articles.\n",
      "  Page 7/65 fetched, 50 articles.\n",
      "  Page 8/65 fetched, 50 articles.\n",
      "  Page 9/65 fetched, 50 articles.\n",
      "  Page 10/65 fetched, 50 articles.\n",
      "  Page 11/65 fetched, 50 articles.\n",
      "  Page 12/65 fetched, 50 articles.\n",
      "  Page 13/65 fetched, 50 articles.\n",
      "  Page 14/65 fetched, 50 articles.\n",
      "  Page 15/65 fetched, 50 articles.\n",
      "  Page 16/65 fetched, 50 articles.\n",
      "  Page 17/65 fetched, 50 articles.\n",
      "  Page 18/65 fetched, 50 articles.\n",
      "  Page 19/65 fetched, 50 articles.\n",
      "  Page 20/65 fetched, 50 articles.\n",
      "  Page 21/65 fetched, 50 articles.\n",
      "  Page 22/65 fetched, 50 articles.\n",
      "  Page 23/65 fetched, 50 articles.\n",
      "  Page 24/65 fetched, 50 articles.\n",
      "  Page 25/65 fetched, 50 articles.\n",
      "  Page 26/65 fetched, 50 articles.\n",
      "  Page 27/65 fetched, 50 articles.\n",
      "  Page 28/65 fetched, 50 articles.\n",
      "  Page 29/65 fetched, 50 articles.\n",
      "  Page 30/65 fetched, 50 articles.\n",
      "  Page 31/65 fetched, 50 articles.\n",
      "  Page 32/65 fetched, 50 articles.\n",
      "  Page 33/65 fetched, 50 articles.\n",
      "  Page 34/65 fetched, 50 articles.\n",
      "  Page 35/65 fetched, 50 articles.\n",
      "  Page 36/65 fetched, 50 articles.\n",
      "  Page 37/65 fetched, 50 articles.\n",
      "  Page 38/65 fetched, 50 articles.\n",
      "  Page 39/65 fetched, 50 articles.\n",
      "  Page 40/65 fetched, 50 articles.\n",
      "  Page 41/65 fetched, 50 articles.\n",
      "  Page 42/65 fetched, 50 articles.\n",
      "  Page 43/65 fetched, 50 articles.\n",
      "  Page 44/65 fetched, 50 articles.\n",
      "  Page 45/65 fetched, 50 articles.\n",
      "  Page 46/65 fetched, 50 articles.\n",
      "  Page 47/65 fetched, 50 articles.\n",
      "  Page 48/65 fetched, 50 articles.\n",
      "  Page 49/65 fetched, 50 articles.\n",
      "  Page 50/65 fetched, 50 articles.\n",
      "  Page 51/65 fetched, 50 articles.\n",
      "  Page 52/65 fetched, 50 articles.\n",
      "  Page 53/65 fetched, 50 articles.\n",
      "  Page 54/65 fetched, 50 articles.\n",
      "  Page 55/65 fetched, 50 articles.\n",
      "  Page 56/65 fetched, 50 articles.\n",
      "  Page 57/65 fetched, 50 articles.\n",
      "  Page 58/65 fetched, 50 articles.\n",
      "  Page 59/65 fetched, 50 articles.\n",
      "  Page 60/65 fetched, 50 articles.\n",
      "  Page 61/65 fetched, 50 articles.\n",
      "  Page 62/65 fetched, 50 articles.\n",
      "  Page 63/65 fetched, 50 articles.\n",
      "  Page 64/65 fetched, 50 articles.\n",
      "  Page 65/65 fetched, 33 articles.\n",
      "Collected 3233 articles from The Guardian.\n"
     ]
    }
   ],
   "source": [
    "query = \"South Korea\"\n",
    "start_date = \"2024-01-01\"\n",
    "end_date = \"2024-12-31\"\n",
    "all_articles = []\n",
    "current_page = 1\n",
    "total_pages = 1 # Start with 1, will update after first call\n",
    "\n",
    "print(\"Fetching from The Guardian...\")\n",
    "while current_page <= total_pages:\n",
    "    data = get_guardian_articles(GUARDIAN_API_KEY, query, start_date, end_date, current_page)\n",
    "\n",
    "    if data and data.get(\"response\"):\n",
    "        response_data = data[\"response\"]\n",
    "        total_pages = response_data.get(\"pages\", 1) # Update total pages\n",
    "        results = response_data.get(\"results\", [])\n",
    "\n",
    "        for item in results:\n",
    "            fields = item.get(\"fields\", {})\n",
    "            all_articles.append({\n",
    "                \"source\": \"The Guardian\",\n",
    "                \"date\": item.get(\"webPublicationDate\", \"\").split(\"T\")[0], # Get just the date part\n",
    "                \"headline\": fields.get(\"headline\", \"\"),\n",
    "                \"snippet\": fields.get(\"trailText\", \"\"),\n",
    "                \"url\": item.get(\"webUrl\", \"\")\n",
    "            })\n",
    "        print(f\"  Page {current_page}/{total_pages} fetched, {len(results)} articles.\")\n",
    "\n",
    "        if current_page >= total_pages:\n",
    "            break # Exit loop if we've fetched all pages\n",
    "\n",
    "        current_page += 1\n",
    "        time.sleep(1) # IMPORTANT: Be polite, wait a second between calls\n",
    "    else:\n",
    "        print(\"  No data or error, stopping.\")\n",
    "        break\n",
    "\n",
    "guardian_df = pd.DataFrame(all_articles)\n",
    "print(f\"Collected {len(guardian_df)} articles from The Guardian.\")\n",
    "# print(guardian_df.head()) # Uncomment to see first few rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         source        date  \\\n",
      "0  The Guardian  2024-12-31   \n",
      "1  The Guardian  2024-12-31   \n",
      "2  The Guardian  2024-12-31   \n",
      "3  The Guardian  2024-12-31   \n",
      "4  The Guardian  2024-12-31   \n",
      "\n",
      "                                            headline  \\\n",
      "0  Trump v the world: Inside the 3 January Guardi...   \n",
      "1  South Korea plane crash investigators turn to ...   \n",
      "2  South Korea plane crash investigations focus o...   \n",
      "3  ‘Sex strikes’ aren’t the feminist win they app...   \n",
      "4  Green light: the boss of GB Railfreight with a...   \n",
      "\n",
      "                                             snippet  \\\n",
      "0  Global leaders pivot to face Trump 2.0. <stron...   \n",
      "1  Experts hope flight recorders will provide ans...   \n",
      "2  Jeju Air flight burst into flames when it hit ...   \n",
      "3  The problem with the 4B movement is that it pl...   \n",
      "4  Teenage trainspotting, plus a passion for the ...   \n",
      "\n",
      "                                                 url  \n",
      "0  https://www.theguardian.com/news/2024/dec/31/t...  \n",
      "1  https://www.theguardian.com/world/2024/dec/31/...  \n",
      "2  https://www.theguardian.com/world/2024/dec/31/...  \n",
      "3  https://www.theguardian.com/commentisfree/2024...  \n",
      "4  https://www.theguardian.com/business/2024/dec/...  \n"
     ]
    }
   ],
   "source": [
    "print(guardian_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The New York Times (Using the API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "NYT_API_KEY = \"GeYfArnC7aG0AugYUYkzcns5VgQG2bqw\"  # <--- MY KEY HERE!\n",
    "NYT_API_URL = \"https://api.nytimes.com/svc/search/v2/articlesearch.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nyt_articles(api_key, query, begin_date, end_date, page=0):\n",
    "    \"\"\"Fetches articles from The NYT Article Search API.\"\"\"\n",
    "    params = {\n",
    "        \"q\": query,\n",
    "        \"api-key\": api_key,\n",
    "        \"begin_date\": begin_date,\n",
    "        \"end_date\": end_date,\n",
    "        \"sort\": \"newest\",\n",
    "        \"fl\": \"headline,abstract,pub_date,web_url\",\n",
    "        \"page\": page\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(NYT_API_URL, params=params)\n",
    "        # Check specifically for 429 error before raising generally\n",
    "        if response.status_code == 429:\n",
    "            print(\"  Hit Rate Limit (429). Waiting for 60 seconds before retrying...\")\n",
    "            time.sleep(60) # Wait a full minute if we hit the limit\n",
    "            response = requests.get(NYT_API_URL, params=params) # Retry once\n",
    "\n",
    "        response.raise_for_status()  # Raise an exception for other bad status codes\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching NYT data: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching from The New York Times...\n",
      "  Page 0 fetched, 10 articles.\n",
      "  Page 1 fetched, 10 articles.\n",
      "  Page 2 fetched, 10 articles.\n",
      "  Page 3 fetched, 10 articles.\n",
      "  Page 4 fetched, 10 articles.\n",
      "  Page 5 fetched, 10 articles.\n",
      "  Page 6 fetched, 10 articles.\n",
      "  Page 7 fetched, 10 articles.\n",
      "  Page 8 fetched, 10 articles.\n",
      "  Page 9 fetched, 10 articles.\n",
      "  Page 10 fetched, 10 articles.\n",
      "  Page 11 fetched, 10 articles.\n",
      "  Page 12 fetched, 10 articles.\n",
      "  Page 13 fetched, 10 articles.\n",
      "  Page 14 fetched, 10 articles.\n",
      "  Page 15 fetched, 10 articles.\n",
      "  Page 16 fetched, 10 articles.\n",
      "  Page 17 fetched, 10 articles.\n",
      "  Page 18 fetched, 10 articles.\n",
      "  Page 19 fetched, 10 articles.\n",
      "  Page 20 fetched, 10 articles.\n",
      "  Page 21 fetched, 10 articles.\n",
      "  Page 22 fetched, 10 articles.\n",
      "  Page 23 fetched, 10 articles.\n",
      "  Page 24 fetched, 10 articles.\n",
      "  Page 25 fetched, 10 articles.\n",
      "  Page 26 fetched, 10 articles.\n",
      "  Page 27 fetched, 10 articles.\n",
      "  Page 28 fetched, 10 articles.\n",
      "  Page 29 fetched, 10 articles.\n",
      "  Page 30 fetched, 10 articles.\n",
      "  Page 31 fetched, 10 articles.\n",
      "  Page 32 fetched, 10 articles.\n",
      "  Page 33 fetched, 10 articles.\n",
      "  Page 34 fetched, 10 articles.\n",
      "  Page 35 fetched, 10 articles.\n",
      "  Page 36 fetched, 10 articles.\n",
      "  Page 37 fetched, 10 articles.\n",
      "  Page 38 fetched, 10 articles.\n",
      "  Page 39 fetched, 10 articles.\n",
      "  Page 40 fetched, 10 articles.\n",
      "  Page 41 fetched, 10 articles.\n",
      "  Page 42 fetched, 10 articles.\n",
      "  Page 43 fetched, 10 articles.\n",
      "  Page 44 fetched, 10 articles.\n",
      "  Page 45 fetched, 10 articles.\n",
      "  Page 46 fetched, 10 articles.\n",
      "  Page 47 fetched, 10 articles.\n",
      "  Page 48 fetched, 10 articles.\n",
      "  Page 49 fetched, 10 articles.\n",
      "  Page 50 fetched, 10 articles.\n",
      "  Page 51 fetched, 10 articles.\n",
      "  Page 52 fetched, 10 articles.\n",
      "  Page 53 fetched, 10 articles.\n",
      "  Page 54 fetched, 10 articles.\n",
      "  Page 55 fetched, 10 articles.\n",
      "  Page 56 fetched, 10 articles.\n",
      "  Page 57 fetched, 10 articles.\n",
      "  Page 58 fetched, 10 articles.\n",
      "  Page 59 fetched, 10 articles.\n",
      "  Page 60 fetched, 10 articles.\n",
      "  Page 61 fetched, 10 articles.\n",
      "  Page 62 fetched, 10 articles.\n",
      "  Page 63 fetched, 10 articles.\n",
      "  Page 64 fetched, 10 articles.\n",
      "  Page 65 fetched, 10 articles.\n",
      "  Page 66 fetched, 10 articles.\n",
      "  Page 67 fetched, 10 articles.\n",
      "  Page 68 fetched, 10 articles.\n",
      "  Page 69 fetched, 10 articles.\n",
      "  Page 70 fetched, 10 articles.\n",
      "  Page 71 fetched, 10 articles.\n",
      "  Page 72 fetched, 10 articles.\n",
      "  Page 73 fetched, 10 articles.\n",
      "  Page 74 fetched, 10 articles.\n",
      "  Page 75 fetched, 10 articles.\n",
      "  Page 76 fetched, 10 articles.\n",
      "  Page 77 fetched, 10 articles.\n",
      "  Page 78 fetched, 10 articles.\n",
      "  Page 79 fetched, 10 articles.\n",
      "  Page 80 fetched, 10 articles.\n",
      "  Page 81 fetched, 10 articles.\n",
      "  Page 82 fetched, 10 articles.\n",
      "  Page 83 fetched, 10 articles.\n",
      "  Page 84 fetched, 10 articles.\n",
      "  Page 85 fetched, 10 articles.\n",
      "  Page 86 fetched, 10 articles.\n",
      "  Page 87 fetched, 10 articles.\n",
      "  Page 88 fetched, 10 articles.\n",
      "  Page 89 fetched, 10 articles.\n",
      "  Page 90 fetched, 10 articles.\n",
      "  Page 91 fetched, 10 articles.\n",
      "  Page 92 fetched, 10 articles.\n",
      "  Page 93 fetched, 10 articles.\n",
      "  Page 94 fetched, 10 articles.\n",
      "  Page 95 fetched, 10 articles.\n",
      "  Page 96 fetched, 10 articles.\n",
      "  Page 97 fetched, 10 articles.\n",
      "  Page 98 fetched, 10 articles.\n",
      "  Page 99 fetched, 10 articles.\n",
      "Collected 1000 articles from The New York Times.\n"
     ]
    }
   ],
   "source": [
    "query = \"South Korea\"\n",
    "start_date = \"20240501\" # Changed to May 1st\n",
    "end_date = \"20250525\"   # Changed to May 25th\n",
    "all_articles_nyt = [] # Use a different name to avoid confusion\n",
    "current_page = 0\n",
    "max_pages = 100\n",
    "\n",
    "print(\"\\nFetching from The New York Times...\")\n",
    "while current_page < max_pages:\n",
    "    # Use the keys you provided (kept private)\n",
    "    data = get_nyt_articles(NYT_API_KEY, query, start_date, end_date, current_page)\n",
    "\n",
    "    if data and data.get(\"response\") and data[\"response\"].get(\"docs\"):\n",
    "        docs = data[\"response\"][\"docs\"]\n",
    "        if not docs:\n",
    "            print(\"  No more articles found.\")\n",
    "            break\n",
    "\n",
    "        for item in docs:\n",
    "            all_articles_nyt.append({\n",
    "                \"source\": \"New York Times\",\n",
    "                \"date\": item.get(\"pub_date\", \"\").split(\"T\")[0],\n",
    "                \"headline\": item.get(\"headline\", {}).get(\"main\", \"\"),\n",
    "                \"snippet\": item.get(\"abstract\", \"\"),\n",
    "                \"url\": item.get(\"web_url\", \"\")\n",
    "            })\n",
    "        print(f\"  Page {current_page} fetched, {len(docs)} articles.\")\n",
    "\n",
    "        current_page += 1\n",
    "        # NEW SLEEP TIME: Wait 13 seconds (60 / 13 is less than 5 calls/min)\n",
    "        time.sleep(13) # IMPORTANT: Increased delay!\n",
    "    else:\n",
    "        # Check for error message in response\n",
    "        if data and data.get(\"message\"):\n",
    "            print(f\"  API Message: {data['message']}\")\n",
    "        elif data and 'fault' in data:\n",
    "            print(f\"  API Fault: {data['fault'].get('faultstring')}\")\n",
    "        elif data and 'status' in data and data['status'] != 'OK':\n",
    "             print(f\"  API Error: {data}\")\n",
    "        else:\n",
    "             print(\"  No data or error, stopping.\")\n",
    "        break\n",
    "\n",
    "nyt_df = pd.DataFrame(all_articles_nyt)\n",
    "print(f\"Collected {len(nyt_df)} articles from The New York Times.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           source        date  \\\n",
      "0  New York Times  2025-05-23   \n",
      "1  New York Times  2025-05-22   \n",
      "2  New York Times  2025-05-22   \n",
      "3  New York Times  2025-05-22   \n",
      "4  New York Times  2025-05-21   \n",
      "\n",
      "                                            headline  \\\n",
      "0  Shock at Harvard After Government Says Interna...   \n",
      "1  Read the Full ‘Make America Healthy Again’ Report   \n",
      "2  Southwest Airlines to Tighten Restrictions on ...   \n",
      "3                                     Formally Picks   \n",
      "4                    Embracing the Soft Power of Art   \n",
      "\n",
      "                                             snippet  \\\n",
      "0  Fear and confusion mounted quickly on Thursday...   \n",
      "1  The White House released an expansive report t...   \n",
      "2  The carrier will require passengers to keep li...   \n",
      "3       David J. Kahn returns with an epic Thursday.   \n",
      "4  Across the globe, more and more cities and cou...   \n",
      "\n",
      "                                                 url  \n",
      "0  https://www.nytimes.com/2025/05/22/us/harvard-...  \n",
      "1  https://www.nytimes.com/interactive/2025/05/22...  \n",
      "2  https://www.nytimes.com/2025/05/22/business/so...  \n",
      "3  https://www.nytimes.com/2025/05/21/crosswords/...  \n",
      "4  https://www.nytimes.com/2025/05/21/arts/soft-p...  \n"
     ]
    }
   ],
   "source": [
    "print(nyt_df.head()) # Uncomment to see first few rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Al Jazeera (Using RSS Feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "NEWS_API_KEY = \"c52200b370334852b307513e88ce870a\" # <--- MY KEY HERE!\n",
    "NEWS_API_URL = \"https://newsapi.org/v2/everything\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching from al-jazeera-english via NewsAPI...\n",
      "  Collected 21 articles.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_newsapi_articles(api_key, query, from_date, to_date, source_id):\n",
    "    \"\"\"Fetches articles from NewsAPI.org for a specific source.\"\"\"\n",
    "    params = {\n",
    "        \"q\": query,\n",
    "        \"apiKey\": api_key,\n",
    "        \"from\": from_date, # Format: 2025-04-26T00:00:00\n",
    "        \"to\": to_date,     # Format: 2025-05-26T12:00:00\n",
    "        \"sources\": source_id, # e.g., 'al-jazeera-english', 'bbc-news', 'abc-news-au'\n",
    "        \"pageSize\": 100, # Max 100\n",
    "        \"language\": \"en\"\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(NEWS_API_URL, params=params)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching NewsAPI data: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- How to use it ---\n",
    "# You MUST check NewsAPI.org for available source IDs.\n",
    "# Let's try 'al-jazeera-english'\n",
    "# REMEMBER: Likely only last 30 days!\n",
    "query = \"South Korea\"\n",
    "# Adjust dates to be within the last 30 days for testing\n",
    "start_date_newsapi = \"2025-04-26T00:00:00\"\n",
    "end_date_newsapi = \"2025-05-26T12:00:00\"\n",
    "source = \"al-jazeera-english\" # Or 'bbc-news', 'abc-news-au', etc.\n",
    "all_newsapi_articles = []\n",
    "\n",
    "print(f\"\\nFetching from {source} via NewsAPI...\")\n",
    "data = get_newsapi_articles(NEWS_API_KEY, query, start_date_newsapi, end_date_newsapi, source)\n",
    "\n",
    "if data and data.get(\"status\") == \"ok\":\n",
    "    articles = data.get(\"articles\", [])\n",
    "    for item in articles:\n",
    "        all_newsapi_articles.append({\n",
    "            \"source\": item.get(\"source\", {}).get(\"name\", source),\n",
    "            \"date\": item.get(\"publishedAt\", \"\").split(\"T\")[0],\n",
    "            \"headline\": item.get(\"title\", \"\"),\n",
    "            \"snippet\": item.get(\"description\", \"\"),\n",
    "            \"url\": item.get(\"url\", \"\")\n",
    "        })\n",
    "    print(f\"  Collected {len(all_newsapi_articles)} articles.\")\n",
    "else:\n",
    "    print(f\"  Could not fetch articles. Status: {data.get('status')}, Message: {data.get('message')}\")\n",
    "\n",
    "newsapi_df = pd.DataFrame(all_newsapi_articles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               source        date  \\\n",
      "0  Al Jazeera English  2025-05-08   \n",
      "1  Al Jazeera English  2025-04-27   \n",
      "2  Al Jazeera English  2025-05-02   \n",
      "3  Al Jazeera English  2025-05-18   \n",
      "4  Al Jazeera English  2025-05-01   \n",
      "\n",
      "                                            headline  \\\n",
      "0  North Korea fires missiles off east coast, Sou...   \n",
      "1  S Korea’s main opposition party taps former ch...   \n",
      "2  South Korea appoints new acting leader as ex-P...   \n",
      "3  South Korea’s presidential candidates hold fir...   \n",
      "4  Former South Korean President Yoon indicted fo...   \n",
      "\n",
      "                                             snippet  \\\n",
      "0  Seoul’s military says launches may have been t...   \n",
      "1  Democratic Party of Korea names Lee Jae-myung ...   \n",
      "2  Han Duck-soo declares candidacy in June 3 elec...   \n",
      "3  Candidates Lee Jae-myung, the frontrunner, and...   \n",
      "4  Yoon Suk-yeol already faces ongoing trial on i...   \n",
      "\n",
      "                                                 url  \n",
      "0  https://www.aljazeera.com/news/2025/5/8/north-...  \n",
      "1  https://www.aljazeera.com/news/2025/4/27/south...  \n",
      "2  https://www.aljazeera.com/news/2025/5/2/south-...  \n",
      "3  https://www.aljazeera.com/news/2025/5/18/south...  \n",
      "4  https://www.aljazeera.com/news/2025/5/1/former...  \n"
     ]
    }
   ],
   "source": [
    "print(newsapi_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Guardian: 3233 articles\n",
    "#### The New York Times: 1000 articles\n",
    "#### Al Jazeera (via NewsAPI): 21 articles\n",
    "#### Total: 4254 articles!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardian DF found: 3233 rows\n",
      "NYT DF found: 1000 rows\n",
      "NewsAPI DF found: 21 rows\n",
      "\n",
      "Total articles combined: 4254\n",
      "\n",
      "Sample of combined data:\n",
      "         source        date  \\\n",
      "0  The Guardian  2024-12-31   \n",
      "1  The Guardian  2024-12-31   \n",
      "2  The Guardian  2024-12-31   \n",
      "3  The Guardian  2024-12-31   \n",
      "4  The Guardian  2024-12-31   \n",
      "\n",
      "                                            headline  \\\n",
      "0  Trump v the world: Inside the 3 January Guardi...   \n",
      "1  South Korea plane crash investigators turn to ...   \n",
      "2  South Korea plane crash investigations focus o...   \n",
      "3  ‘Sex strikes’ aren’t the feminist win they app...   \n",
      "4  Green light: the boss of GB Railfreight with a...   \n",
      "\n",
      "                                             snippet  \\\n",
      "0  Global leaders pivot to face Trump 2.0. <stron...   \n",
      "1  Experts hope flight recorders will provide ans...   \n",
      "2  Jeju Air flight burst into flames when it hit ...   \n",
      "3  The problem with the 4B movement is that it pl...   \n",
      "4  Teenage trainspotting, plus a passion for the ...   \n",
      "\n",
      "                                                 url  \n",
      "0  https://www.theguardian.com/news/2024/dec/31/t...  \n",
      "1  https://www.theguardian.com/world/2024/dec/31/...  \n",
      "2  https://www.theguardian.com/world/2024/dec/31/...  \n",
      "3  https://www.theguardian.com/commentisfree/2024...  \n",
      "4  https://www.theguardian.com/business/2024/dec/...  \n",
      "\n",
      "DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4254 entries, 0 to 4253\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   source    4254 non-null   object\n",
      " 1   date      4254 non-null   object\n",
      " 2   headline  4254 non-null   object\n",
      " 3   snippet   4254 non-null   object\n",
      " 4   url       4254 non-null   object\n",
      "dtypes: object(5)\n",
      "memory usage: 166.3+ KB\n",
      "\n",
      "Successfully saved combined data to 'south_korea_news_all.csv'\n"
     ]
    }
   ],
   "source": [
    "dfs_to_combine = []\n",
    "if 'guardian_df' in locals():\n",
    "    dfs_to_combine.append(guardian_df)\n",
    "    print(f\"Guardian DF found: {len(guardian_df)} rows\")\n",
    "if 'nyt_df' in locals():\n",
    "    dfs_to_combine.append(nyt_df)\n",
    "    print(f\"NYT DF found: {len(nyt_df)} rows\")\n",
    "if 'newsapi_df' in locals():\n",
    "    dfs_to_combine.append(newsapi_df)\n",
    "    print(f\"NewsAPI DF found: {len(newsapi_df)} rows\")\n",
    "\n",
    "if dfs_to_combine:\n",
    "    # Combine them all\n",
    "    all_news_df = pd.concat(dfs_to_combine, ignore_index=True)\n",
    "\n",
    "    print(f\"\\nTotal articles combined: {len(all_news_df)}\")\n",
    "\n",
    "    # Display a sample and check info\n",
    "    print(\"\\nSample of combined data:\")\n",
    "    print(all_news_df.head())\n",
    "    print(\"\\nDataFrame Info:\")\n",
    "    all_news_df.info()\n",
    "\n",
    "    # Save the master dataset to a CSV file\n",
    "    try:\n",
    "        all_news_df.to_csv(\"south_korea_news_all.csv\", index=False)\n",
    "        print(\"\\nSuccessfully saved combined data to 'south_korea_news_all.csv'\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError saving CSV: {e}\")\n",
    "else:\n",
    "    print(\"Could not find DataFrames to combine. Please check your variables.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
